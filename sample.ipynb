{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar etiquetas de entrenamiento (archivo CSV con columnas \"image\",\"count\")\n",
    "df = pd.read_csv('C:\\\\Admin\\Downloads\\personas\\part_B_final\\train_data.csv')  # ✅ Agregar ruta correcta\n",
    "image_paths = ['train_images/' + fname for fname in df['image']]\n",
    "counts = df['count'].values\n",
    "\n",
    "# Función para leer y preprocesar una imagen\n",
    "def load_image(path, count):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (128, 128))\n",
    "    img = img / 255.0\n",
    "    return img, tf.cast(count, tf.float32)\n",
    "\n",
    "# Crear Dataset tf.data a partir de los paths y conteos\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, counts))\n",
    "dataset = dataset.map(load_image).shuffle(500).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ✅ MEJORAR: División de datos más robusta\n",
    "train_size = int(0.8 * len(df))\n",
    "# Calcular número de batches correctamente\n",
    "train_batches = train_size // 16\n",
    "val_batches = (len(df) - train_size) // 16\n",
    "\n",
    "train_dataset = dataset.take(train_batches)\n",
    "val_dataset = dataset.skip(train_batches).take(val_batches)\n",
    "\n",
    "# ✅ MEJORAR: Modelo CNN más robusto\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),  # Más filtros\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),  # Capa adicional\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),  # Mejor que Flatten\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Más neuronas\n",
    "    tf.keras.layers.Dropout(0.5),  # Prevenir overfitting\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1)  # Capa de salida: un solo valor (conteo)\n",
    "])\n",
    "\n",
    "# ✅ MEJORAR: Configuración de compilación\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# ✅ AGREGAR: Callbacks para mejor entrenamiento\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mae',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_mae',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_mae',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entrenar el modelo con más epochs y callbacks\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,  # Más epochs con early stopping\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ✅ CORREGIR: Preparar imágenes de prueba con ruta correcta\n",
    "# Verificar primero si existe el archivo test_labels.csv o crear lista de imágenes\n",
    "import os\n",
    "\n",
    "# Opción A: Si tienes test_labels.csv en sample_data\n",
    "if os.path.exists('sample_data/test_labels.csv'):\n",
    "    test_df = pd.read_csv('sample_data/test_labels.csv')\n",
    "    test_paths = ['test_images/' + fname for fname in test_df['image']]\n",
    "\n",
    "# Opción B: Si no tienes CSV, leer directamente los archivos de test_images\n",
    "else:\n",
    "    test_files = [f for f in os.listdir('test_images/') if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    test_paths = ['test_images/' + fname for fname in test_files]\n",
    "    test_df = pd.DataFrame({'image': test_files})\n",
    "\n",
    "print(f\"Encontradas {len(test_paths)} imágenes de prueba\")\n",
    "\n",
    "# Crear dataset de prueba\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "test_ds = test_ds.map(lambda x: tf.image.resize(\n",
    "    tf.image.decode_jpeg(tf.io.read_file(x), channels=3), (128,128)) / 255.0).batch(16)\n",
    "\n",
    "# Predecir los conteos en el conjunto de prueba\n",
    "print(\"Generando predicciones...\")\n",
    "predictions = model.predict(test_ds)\n",
    "predictions = tf.squeeze(predictions).numpy()\n",
    "\n",
    "# ✅ MEJORAR: Validar predicciones\n",
    "# Asegurar que no hay valores negativos\n",
    "predictions = np.maximum(predictions, 0)\n",
    "\n",
    "# Crear archivo de envío (CSV)\n",
    "submission = pd.DataFrame({\n",
    "    'image': test_df['image'],\n",
    "    'predicted_count': np.round(predictions).astype(int)\n",
    "})\n",
    "\n",
    "# Mostrar estadísticas de las predicciones\n",
    "print(f\"Estadísticas de predicciones:\")\n",
    "print(f\"Min: {submission['predicted_count'].min()}\")\n",
    "print(f\"Max: {submission['predicted_count'].max()}\")\n",
    "print(f\"Promedio: {submission['predicted_count'].mean():.2f}\")\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Archivo submission.csv creado exitosamente!\")\n",
    "\n",
    "# ✅ OPCIONAL: Visualizar algunas predicciones\n",
    "print(\"\\nPrimeras 10 predicciones:\")\n",
    "print(submission.head(10))"
   ],
   "id": "fbc121e30a2defb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
